{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bca2c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.fftpack import dct\n",
    "from silero_vad import load_silero_vad, read_audio, get_speech_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36ed512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the audio samples are single-channel(mono)\n",
    "def load_audio(path):\n",
    "    sampleRate, audio = scipy.io.wavfile.read(path)\n",
    "    audio=audio[0:int(3.5*sampleRate)]\n",
    "    sampleRate=audio.size\n",
    "    return sampleRate,audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f119d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_audio_init(audio):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(audio)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(\"Audio Signal\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db3aaeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vad(sampleRate,audio,path):\n",
    "    model = load_silero_vad()\n",
    "    wav = read_audio(path)\n",
    "    speech_timestamps = get_speech_timestamps(wav,model)\n",
    "    try:\n",
    "        x=speech_timestamps[0]['start']\n",
    "    except:\n",
    "        return -1,audio\n",
    "    if speech_timestamps[0]['start']+6000>sampleRate:\n",
    "        sampleRate=-1\n",
    "        return sampleRate,audio\n",
    "    else:\n",
    "        speech_timestamps[0]['start']+6000\n",
    "    audio=audio[speech_timestamps[0]['start']:speech_timestamps[0]['start']+6000]\n",
    "    sampleRate=audio.size\n",
    "    return sampleRate,audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9064e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time vs Amplitude Graph of wav file\n",
    "def plot_audio_vad(audio):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(audio)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(\"Audio Signal after VAD\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b591725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Containing and isolating only speech by removing silence or hums.\n",
    "# discard_threshold=max(audio)*0.15\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "# ax1.plot(audio)\n",
    "# boundLeft=0\n",
    "# for i in range(0,sampleRate):\n",
    "#     if(audio[i]>=discard_threshold):\n",
    "#         if(i>1000):\n",
    "#             boundLeft=i-1000\n",
    "#         else:\n",
    "#             boundLeft=i\n",
    "#         break\n",
    "# boundRight=0\n",
    "# for i in range(sampleRate-1,-1,-1):\n",
    "#     if(audio[i]>=discard_threshold):\n",
    "#         if(i<15000):\n",
    "#             boundRight=i+1000\n",
    "#         else:\n",
    "#             boundRight=i\n",
    "#         break\n",
    "\n",
    "# audio=audio[boundLeft:boundRight]\n",
    "# sampleRate=audio.size\n",
    "# plt.plot(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ce5cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate MFCC, we follow the following steps:\n",
    "# audioInput -> pre-emphasis -> framing -> windowing -> fourier transform -> Inverse Mel Scale Filter Bank -> Log() -> DCT ->\n",
    "# Derivatives -> Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba6e6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-emphasis layer\n",
    "# Amplifies higher frequencies in order to balance the spectrum (higher frequencies have lower energies)\n",
    "def pre_emphasize(sampleRate,audio):\n",
    "    pre_emphasis = 0.97\n",
    "    audio_preemphasized=[]\n",
    "    for i in range(1,sampleRate):\n",
    "        audio_preemphasized=np.append(audio_preemphasized,audio[i]-(audio[i-1]*pre_emphasis))\n",
    "\n",
    "    return audio_preemphasized\n",
    "\n",
    "def plot_audio_pre_emphasis(audio_preemphasized):\n",
    "    # Plot the pre-emphasized signal\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(audio_preemphasized)\n",
    "    plt.title('Pre-emphasized Signal')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0de961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framing Layer\n",
    "# Since the audio wave is more than a second, windowing is necesarry in order to fully capture the features and allow for correct\n",
    "# calculations to be performed. Thus, for ease of calculations, we slice the wave.\n",
    "# The signal/wave is separated into sections or frames of 25-30 milliseconds.\n",
    "# Since some parts of the signal are always at the ends of the frames, and we have to perform hamming window, this may result in data loss.\n",
    "# To tackle this, we frame-shift with a stride of 15ms. This ensures that parts of signals get to be in the center of the signal.\n",
    "\n",
    "def frame_audio(sampleRate,audio_preemphasized):\n",
    "    shift_stride=220  # ~10 millisecond of stride\n",
    "    frame_size=650 # ~30 millisecond frame\n",
    "    audio_frames=[]\n",
    "\n",
    "    # Produces 65 audio frames\n",
    "    for i in range(0,sampleRate-frame_size,shift_stride):\n",
    "        audio_frames.append(audio_preemphasized[i:i+frame_size])\n",
    "\n",
    "    return frame_size,audio_frames\n",
    "\n",
    "def plot_audio_frame(audio_frames):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(audio_frames[2])\n",
    "    plt.title('Framed Signal')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4264e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windowing Layer\n",
    "# Since sudden increase/decrease of amplitude at the edges of the frames create noisy outcomes, we have to smoothen it.\n",
    "# Thus, we apply hamming window\n",
    "\n",
    "def window_audio(sampleRate,frame_size,audio_frames):\n",
    "    hammed_audio=[]\n",
    "    for frame in audio_frames:\n",
    "        temp_hammed_audio=[]\n",
    "        for i in range(0,frame_size):\n",
    "            temp_hammed_audio.append(frame[i]*(0.54-0.46*math.cos(2*math.pi*i/(frame_size-1))))\n",
    "        \n",
    "        hammed_audio.append(temp_hammed_audio)\n",
    "\n",
    "    return hammed_audio\n",
    "\n",
    "def plot_audio_hammed(hammed_audio):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(hammed_audio[2])\n",
    "    plt.title('Windowed Signal')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ad7764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT(Fast Fourier Transform) Layer\n",
    "# Used to convert time-domain signal to frequency-domain to analyze frequency components of speech.\n",
    "# Output of FFT gives complex frequency spectrum (both magnitude and phase)\n",
    "# Since we only need magnitude, we evaluate the power spectrum from the output of FFT\n",
    "# NFFT specifies number of points for the FFT. The output is NFFT/2 points\n",
    "\n",
    "def pow_spec(hammed_audio):\n",
    "    NFFT=2048\n",
    "    complex_power_spectrums=np.fft.rfft(hammed_audio,NFFT)\n",
    "    power_spectrum=(1/NFFT)*pow(np.abs(complex_power_spectrums),2)\n",
    "    return NFFT,power_spectrum\n",
    "\n",
    "def plot_power_spectrum(power_spectrum):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(power_spectrum[2])\n",
    "    plt.title(\"Power Spectral Density\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Power/Frequency (dB/Hz)\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61e87381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mel-filter banks\n",
    "# Mel-scale related to human-percieved frequency to its actual frequency. Since humans do not hear sound linearly,\n",
    "# i.e, linear gaps in frequency does not amount to linear change in pitch, we use mel-scale.\n",
    "# Mel-scale is a logarithm scale, which imitates hearing of humans. Thus, it enables us to capture features as if heard by human.\n",
    "\n",
    "# Computing the Mel-Filter bank\n",
    "# 1. Decide upper and lower frequencies in Hertz(SampleRate/2 and 300Hz repectively) \n",
    "# 2. Convert them to mels.\n",
    "# 3. Compute 12 linearly-spaced frequencies inclusive of lower and upper mels.\n",
    "# 4. Convert these points back to Hertz.\n",
    "# 5. Round the frequencies to their nearest FFT Bins.\n",
    "# 6. Create Filterbanks\n",
    "\n",
    "def mels(sampleRate,NFFT):\n",
    "    mels=0\n",
    "    freq_to_mel=lambda freq:1125*math.log(1+freq/700)\n",
    "    lower_hz=300\n",
    "    upper_hz=sampleRate/2\n",
    "\n",
    "    lower_mel=freq_to_mel(lower_hz)\n",
    "    upper_mel=freq_to_mel(upper_hz)\n",
    "\n",
    "    n_filters=40\n",
    "    mel_arr=np.linspace(lower_mel,upper_mel,n_filters+2)\n",
    "    hz_arr=[700*(math.exp((i/1125))-1) for i in mel_arr]\n",
    "\n",
    "    freq_bin=[math.floor((NFFT+1)*hz_arr_i/sampleRate) for hz_arr_i in hz_arr]\n",
    "    return n_filters,freq_bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d79fb766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the filterbanks\n",
    "\n",
    "def mel_filterbanks(NFFT,n_filters,freq_bin,power_spectrum):\n",
    "    temp_filter_bank=np.zeros((n_filters,int((NFFT/2))+1))\n",
    "    for i in range(1,n_filters+1):\n",
    "        for k in range(0,int((NFFT/2))):  #frame length\n",
    "            if k<freq_bin[i]:\n",
    "                temp_filter_bank[i-1][k]=0\n",
    "            elif freq_bin[i-1]<=k and k<=freq_bin[i]:\n",
    "                temp_filter_bank[i-1][k]=(k-freq_bin[i-1])/(freq_bin[i]-freq_bin[i-1])\n",
    "            elif freq_bin[i]<=k and k<=freq_bin[i+1]:\n",
    "                temp_filter_bank[i-1][k]=(freq_bin[i+1]-k)/(freq_bin[i+1]-freq_bin[i])\n",
    "            else:\n",
    "                temp_filter_bank[i-1][k]=0\n",
    "\n",
    "\n",
    "    filter_banks=np.dot(power_spectrum, temp_filter_bank.T)\n",
    "    filter_banks=np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)\n",
    "    filter_banks=np.log(filter_banks+1e-8)\n",
    "\n",
    "    return filter_banks\n",
    "\n",
    "def plot_mel_spectogram(sampleRate,filter_banks):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.specshow(filter_banks.T, sr=sampleRate, x_axis='time', y_axis='mel',cmap='turbo')\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60940534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate MFCCs\n",
    "# We apply DCT on the filterbanks to obtain a set of 26 Mel-Frequency Cepstral Coefficients.\n",
    "# We only require first 13 coefficients for ASR purposes. Rest are to be discarded.\n",
    "\n",
    "def gen_mfcc(filter_banks):\n",
    "    mfcc = dct(filter_banks, type=2, axis=1)[:, 1:13] # Keep 2-13\n",
    "    return mfcc\n",
    "\n",
    "def plot_mfcc(sampleRate,mfcc):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.specshow(mfcc.T, sr=sampleRate, x_axis='time', y_axis='mel',cmap='turbo',vmin=-100,vmax=100)\n",
    "    plt.colorbar()\n",
    "    plt.ylabel(\"mfcc coeff\")\n",
    "    plt.tight_layout()\n",
    "    s=f\"C:/Users/svija/Downloads/testmfcc.png\"\n",
    "    plt.savefig(s)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed2e0e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(path):\n",
    "    flag=0\n",
    "    sampleRate,audio=load_audio(path)\n",
    "    sampleRate,audio=vad(sampleRate,audio,path)\n",
    "    if sampleRate==-1:\n",
    "        flag=1\n",
    "        return flag,flag\n",
    "    audio_preemphasized=pre_emphasize(sampleRate,audio)\n",
    "    frame_size,audio_frames=frame_audio(sampleRate,audio_preemphasized)\n",
    "    hammed_audio=window_audio(sampleRate,frame_size,audio_frames)\n",
    "    NFFT,power_spectrum=pow_spec(hammed_audio)\n",
    "    n_filters,freq_bin=mels(sampleRate,NFFT)\n",
    "    filter_banks=mel_filterbanks(NFFT,n_filters,freq_bin,power_spectrum)\n",
    "    mfcc=gen_mfcc(filter_banks)\n",
    "    return flag,mfcc\n",
    "\n",
    "def get_mel_filterbanks(path):\n",
    "    sampleRate,audio=load_audio(path)\n",
    "    sampleRate,audio=vad(sampleRate,audio,path)\n",
    "    audio_preemphasized=pre_emphasize(sampleRate,audio)\n",
    "    frame_size,audio_frames=frame_audio(sampleRate,audio_preemphasized)\n",
    "    hammed_audio=window_audio(sampleRate,frame_size,audio_frames)\n",
    "    NFFT,power_spectrum=pow_spec(hammed_audio)\n",
    "    n_filters,freq_bin=mels(sampleRate,NFFT)\n",
    "    filter_banks=mel_filterbanks(NFFT,n_filters,freq_bin,power_spectrum)\n",
    "    return filter_banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5116dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8d3629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_path=\"C:/Main/Dev/Github/Py-STT-Engine-venv/Py-STT-Engine/audioData/Animals/dog/\"\n",
    "# file_names = os.listdir(files_path)\n",
    "# for file in file_names:\n",
    "#     flag,mfcc=get_mfcc(files_path+file)\n",
    "#     if flag==1:\n",
    "#         continue\n",
    "#     plt.figure(figsize=(12, 4))\n",
    "#     librosa.display.specshow(mfcc.T, x_axis='time', y_axis='mel',cmap='turbo',vmin=-100,vmax=100)\n",
    "#     plt.colorbar()\n",
    "#     plt.ylabel(\"mfcc coeff\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(\"C:/Main/Dev/Github/Py-STT-Engine-venv/Py-STT-Engine/mfccs_all/dog/\"+file+\".png\")\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3836ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48baf03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image=cv2.imread(\"C:/users/svija/downloads/testmfcc.png\")\n",
    "cv2.imshow(\"MFCC\",image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba9c99c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 1200, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0e0c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Neural Network to detect word.\n",
    "# Traditionally, we use neural networks to detect phonemes.\n",
    "# However, just for experiment and learning purposes, I will apply a CNN to the MFCCs for ternary classification between the words\n",
    "# \"bird\", \"cat\", \"dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0019c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPool2D,Flatten,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77fc504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "#400x1200\n",
    "model.add(Conv2D(8,(3,3),activation='relu',input_shape=(400,1200,3),padding='valid'))\n",
    "model.add(MaxPool2D((2,2),strides=(2, 2),padding='valid'))\n",
    "\n",
    "#200x600\n",
    "model.add(Conv2D(16,(3,3),activation='relu',padding='valid'))\n",
    "model.add(MaxPool2D((2,2),strides=(2, 2),padding='valid'))\n",
    "\n",
    "#100x300\n",
    "model.add(Conv2D(16,(3,3),activation='relu',padding='valid'))\n",
    "model.add(MaxPool2D((2,2),padding='valid'))\n",
    "\n",
    "#50x150\n",
    "model.add(Conv2D(16,(3,3),activation='relu',padding='valid'))\n",
    "model.add(MaxPool2D((2,2),padding='valid'))\n",
    "\n",
    "#25x75\n",
    "model.add(Conv2D(16,(3,3),activation='relu',padding='valid'))\n",
    "model.add(MaxPool2D((2,2),padding='valid'))\n",
    "\n",
    "#5625-1Darray\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5625,activation='relu'))\n",
    "\n",
    "model.add(Dense(300,activation='relu'))\n",
    "\n",
    "model.add(Dense(3,activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60d310ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2cae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path=\"C:/Main/Dev/Github/Py-STT-Engine-venv/Py-STT-Engine/bird\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
