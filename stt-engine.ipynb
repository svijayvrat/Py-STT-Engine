{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112f388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.fftpack import dct\n",
    "from silero_vad import load_silero_vad, read_audio, get_speech_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f03d8f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the audio samples are single-channel(mono)\n",
    "def load_audio(path):\n",
    "    sampleRate, audio = scipy.io.wavfile.read(path)\n",
    "    audio=audio[0:int(3.5*sampleRate)]\n",
    "    sampleRate=audio.size\n",
    "    return sampleRate,audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c084e08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_audio_init(audio):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(audio)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(\"Audio Signal\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e44e6c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vad(sampleRate,audio,path):\n",
    "    model = load_silero_vad()\n",
    "    wav = read_audio(path)\n",
    "    speech_timestamps = get_speech_timestamps(wav,model)\n",
    "    try:\n",
    "        x=speech_timestamps[0]['start']\n",
    "    except:\n",
    "        return -1,audio\n",
    "    if speech_timestamps[0]['start']+6000>sampleRate:\n",
    "        sampleRate=-1\n",
    "        return sampleRate,audio\n",
    "    else:\n",
    "        speech_timestamps[0]['start']+6000\n",
    "    audio=audio[speech_timestamps[0]['start']:speech_timestamps[0]['start']+6000]\n",
    "    sampleRate=audio.size\n",
    "    return sampleRate,audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38675c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time vs Amplitude Graph of wav file\n",
    "def plot_audio_vad(audio):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(audio)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(\"Audio Signal after VAD\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "504fba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Containing and isolating only speech by removing silence or hums.\n",
    "# discard_threshold=max(audio)*0.15\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "# ax1.plot(audio)\n",
    "# boundLeft=0\n",
    "# for i in range(0,sampleRate):\n",
    "#     if(audio[i]>=discard_threshold):\n",
    "#         if(i>1000):\n",
    "#             boundLeft=i-1000\n",
    "#         else:\n",
    "#             boundLeft=i\n",
    "#         break\n",
    "# boundRight=0\n",
    "# for i in range(sampleRate-1,-1,-1):\n",
    "#     if(audio[i]>=discard_threshold):\n",
    "#         if(i<15000):\n",
    "#             boundRight=i+1000\n",
    "#         else:\n",
    "#             boundRight=i\n",
    "#         break\n",
    "\n",
    "# audio=audio[boundLeft:boundRight]\n",
    "# sampleRate=audio.size\n",
    "# plt.plot(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75ee9625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate MFCC, we follow the following steps:\n",
    "# audioInput -> pre-emphasis -> framing -> windowing -> fourier transform -> Inverse Mel Scale Filter Bank -> Log() -> DCT ->\n",
    "# Derivatives -> Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac0d807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-emphasis layer\n",
    "# Amplifies higher frequencies in order to balance the spectrum (higher frequencies have lower energies)\n",
    "def pre_emphasize(sampleRate,audio):\n",
    "    pre_emphasis = 0.97\n",
    "    audio_preemphasized=[]\n",
    "    for i in range(1,sampleRate):\n",
    "        audio_preemphasized=np.append(audio_preemphasized,audio[i]-(audio[i-1]*pre_emphasis))\n",
    "\n",
    "    return audio_preemphasized\n",
    "\n",
    "def plot_audio_pre_emphasis(audio_preemphasized):\n",
    "    # Plot the pre-emphasized signal\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(audio_preemphasized)\n",
    "    plt.title('Pre-emphasized Signal')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52e43b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framing Layer\n",
    "# Since the audio wave is more than a second, windowing is necesarry in order to fully capture the features and allow for correct\n",
    "# calculations to be performed. Thus, for ease of calculations, we slice the wave.\n",
    "# The signal/wave is separated into sections or frames of 25-30 milliseconds.\n",
    "# Since some parts of the signal are always at the ends of the frames, and we have to perform hamming window, this may result in data loss.\n",
    "# To tackle this, we frame-shift with a stride of 15ms. This ensures that parts of signals get to be in the center of the signal.\n",
    "\n",
    "def frame_audio(sampleRate,audio_preemphasized):\n",
    "    shift_stride=220  # ~10 millisecond of stride\n",
    "    frame_size=650 # ~30 millisecond frame\n",
    "    audio_frames=[]\n",
    "\n",
    "    # Produces 65 audio frames\n",
    "    for i in range(0,sampleRate-frame_size,shift_stride):\n",
    "        audio_frames.append(audio_preemphasized[i:i+frame_size])\n",
    "\n",
    "    return frame_size,audio_frames\n",
    "\n",
    "def plot_audio_frame(audio_frames):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(audio_frames[2])\n",
    "    plt.title('Framed Signal')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d03feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windowing Layer\n",
    "# Since sudden increase/decrease of amplitude at the edges of the frames create noisy outcomes, we have to smoothen it.\n",
    "# Thus, we apply hamming window\n",
    "\n",
    "def window_audio(sampleRate,frame_size,audio_frames):\n",
    "    hammed_audio=[]\n",
    "    for frame in audio_frames:\n",
    "        temp_hammed_audio=[]\n",
    "        for i in range(0,frame_size):\n",
    "            temp_hammed_audio.append(frame[i]*(0.54-0.46*math.cos(2*math.pi*i/(frame_size-1))))\n",
    "        \n",
    "        hammed_audio.append(temp_hammed_audio)\n",
    "\n",
    "    return hammed_audio\n",
    "\n",
    "def plot_audio_hammed(hammed_audio):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(hammed_audio[2])\n",
    "    plt.title('Windowed Signal')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e10c027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT(Fast Fourier Transform) Layer\n",
    "# Used to convert time-domain signal to frequency-domain to analyze frequency components of speech.\n",
    "# Output of FFT gives complex frequency spectrum (both magnitude and phase)\n",
    "# Since we only need magnitude, we evaluate the power spectrum from the output of FFT\n",
    "# NFFT specifies number of points for the FFT. The output is NFFT/2 points\n",
    "\n",
    "def pow_spec(hammed_audio):\n",
    "    NFFT=2048\n",
    "    complex_power_spectrums=np.fft.rfft(hammed_audio,NFFT)\n",
    "    power_spectrum=(1/NFFT)*pow(np.abs(complex_power_spectrums),2)\n",
    "    return NFFT,power_spectrum\n",
    "\n",
    "def plot_power_spectrum(power_spectrum):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(power_spectrum[2])\n",
    "    plt.title(\"Power Spectral Density\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Power/Frequency (dB/Hz)\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2e8ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mel-filter banks\n",
    "# Mel-scale related to human-percieved frequency to its actual frequency. Since humans do not hear sound linearly,\n",
    "# i.e, linear gaps in frequency does not amount to linear change in pitch, we use mel-scale.\n",
    "# Mel-scale is a logarithm scale, which imitates hearing of humans. Thus, it enables us to capture features as if heard by human.\n",
    "\n",
    "# Computing the Mel-Filter bank\n",
    "# 1. Decide upper and lower frequencies in Hertz(SampleRate/2 and 300Hz repectively) \n",
    "# 2. Convert them to mels.\n",
    "# 3. Compute 12 linearly-spaced frequencies inclusive of lower and upper mels.\n",
    "# 4. Convert these points back to Hertz.\n",
    "# 5. Round the frequencies to their nearest FFT Bins.\n",
    "# 6. Create Filterbanks\n",
    "\n",
    "def mels(sampleRate,NFFT):\n",
    "    mels=0\n",
    "    freq_to_mel=lambda freq:1125*math.log(1+freq/700)\n",
    "    lower_hz=300\n",
    "    upper_hz=sampleRate/2\n",
    "\n",
    "    lower_mel=freq_to_mel(lower_hz)\n",
    "    upper_mel=freq_to_mel(upper_hz)\n",
    "\n",
    "    n_filters=40\n",
    "    mel_arr=np.linspace(lower_mel,upper_mel,n_filters+2)\n",
    "    hz_arr=[700*(math.exp((i/1125))-1) for i in mel_arr]\n",
    "\n",
    "    freq_bin=[math.floor((NFFT+1)*hz_arr_i/sampleRate) for hz_arr_i in hz_arr]\n",
    "    return n_filters,freq_bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a56fa591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the filterbanks\n",
    "\n",
    "def mel_filterbanks(NFFT,n_filters,freq_bin,power_spectrum):\n",
    "    temp_filter_bank=np.zeros((n_filters,int((NFFT/2))+1))\n",
    "    for i in range(1,n_filters+1):\n",
    "        for k in range(0,int((NFFT/2))):  #frame length\n",
    "            if k<freq_bin[i]:\n",
    "                temp_filter_bank[i-1][k]=0\n",
    "            elif freq_bin[i-1]<=k and k<=freq_bin[i]:\n",
    "                temp_filter_bank[i-1][k]=(k-freq_bin[i-1])/(freq_bin[i]-freq_bin[i-1])\n",
    "            elif freq_bin[i]<=k and k<=freq_bin[i+1]:\n",
    "                temp_filter_bank[i-1][k]=(freq_bin[i+1]-k)/(freq_bin[i+1]-freq_bin[i])\n",
    "            else:\n",
    "                temp_filter_bank[i-1][k]=0\n",
    "\n",
    "\n",
    "    filter_banks=np.dot(power_spectrum, temp_filter_bank.T)\n",
    "    filter_banks=np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)\n",
    "    filter_banks=np.log(filter_banks+1e-8)\n",
    "\n",
    "    return filter_banks\n",
    "\n",
    "def plot_mel_spectogram(sampleRate,filter_banks):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.specshow(filter_banks.T, sr=sampleRate, x_axis='time', y_axis='mel',cmap='turbo')\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "063d7847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate MFCCs\n",
    "# We apply DCT on the filterbanks to obtain a set of 26 Mel-Frequency Cepstral Coefficients.\n",
    "# We only require first 13 coefficients for ASR purposes. Rest are to be discarded.\n",
    "\n",
    "def gen_mfcc(filter_banks):\n",
    "    mfcc = dct(filter_banks, type=2, axis=1)[:, 1:13] # Keep 2-13\n",
    "    return mfcc\n",
    "\n",
    "def plot_mfcc(sampleRate,mfcc):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.specshow(mfcc.T, sr=sampleRate, x_axis='time', y_axis='mel',cmap='turbo',vmin=-100,vmax=100)\n",
    "    plt.colorbar()\n",
    "    plt.ylabel(\"mfcc coeff\")\n",
    "    plt.tight_layout()\n",
    "    s=f\"C:/Users/svija/Downloads/testmfcc.png\"\n",
    "    plt.savefig(s)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5879e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(path):\n",
    "    flag=0\n",
    "    sampleRate,audio=load_audio(path)\n",
    "    sampleRate,audio=vad(sampleRate,audio,path)\n",
    "    if sampleRate==-1:\n",
    "        flag=1\n",
    "        return flag,flag\n",
    "    audio_preemphasized=pre_emphasize(sampleRate,audio)\n",
    "    frame_size,audio_frames=frame_audio(sampleRate,audio_preemphasized)\n",
    "    hammed_audio=window_audio(sampleRate,frame_size,audio_frames)\n",
    "    NFFT,power_spectrum=pow_spec(hammed_audio)\n",
    "    n_filters,freq_bin=mels(sampleRate,NFFT)\n",
    "    filter_banks=mel_filterbanks(NFFT,n_filters,freq_bin,power_spectrum)\n",
    "    mfcc=gen_mfcc(filter_banks)\n",
    "    return flag,mfcc\n",
    "\n",
    "def get_mel_filterbanks(path):\n",
    "    sampleRate,audio=load_audio(path)\n",
    "    sampleRate,audio=vad(sampleRate,audio,path)\n",
    "    audio_preemphasized=pre_emphasize(sampleRate,audio)\n",
    "    frame_size,audio_frames=frame_audio(sampleRate,audio_preemphasized)\n",
    "    hammed_audio=window_audio(sampleRate,frame_size,audio_frames)\n",
    "    NFFT,power_spectrum=pow_spec(hammed_audio)\n",
    "    n_filters,freq_bin=mels(sampleRate,NFFT)\n",
    "    filter_banks=mel_filterbanks(NFFT,n_filters,freq_bin,power_spectrum)\n",
    "    return filter_banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2052482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1687d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(input_path,output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    file_names = os.listdir(input_path)\n",
    "    for file in file_names:\n",
    "        flag,mfcc=get_mfcc(input_path+file)\n",
    "        if flag==1:\n",
    "            continue\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        librosa.display.specshow(mfcc.T, x_axis='time', y_axis='mel',cmap='turbo',vmin=-100,vmax=100)\n",
    "        plt.colorbar()\n",
    "        plt.ylabel(\"mfcc coeff\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path+file+\".png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c523c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svija\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\silero_vad\\utils_vad.py:139: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  list_backends = torchaudio.list_audio_backends()\n",
      "C:\\Users\\svija\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\silero_vad\\utils_vad.py:150: UserWarning: torchaudio.sox_effects.sox_effects.apply_effects_file has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  wav, sr = torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n",
      "C:\\Users\\svija\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#For audio containing word \"dog\"\n",
    "input_path=\"C:/Main/Dev/Github/Py-STT-Engine-venv/Py-STT-Engine/audioData/Animals/dog/\"\n",
    "output_path=\"C:/Main/Dev/Github/Py-STT-Engine-venv/Py-STT-Engine/mfccs_all/dog/\"\n",
    "\n",
    "prepare_training_data(input_path,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97338f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svija\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\silero_vad\\utils_vad.py:139: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  list_backends = torchaudio.list_audio_backends()\n",
      "C:\\Users\\svija\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\silero_vad\\utils_vad.py:150: UserWarning: torchaudio.sox_effects.sox_effects.apply_effects_file has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  wav, sr = torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n",
      "C:\\Users\\svija\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#For audio containing word \"cat\"\n",
    "input_path=\"C:/Main/Dev/Github/Py-STT-Engine-venv/Py-STT-Engine/audioData/Animals/cat/\"\n",
    "output_path=\"C:/Main/Dev/Github/Py-STT-Engine-venv/Py-STT-Engine/mfccs_all/cat/\"\n",
    "\n",
    "prepare_training_data(input_path,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bb54676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For audio containing word \"bird\"\n",
    "input_path=\"C:/Main/Dev/Github/Py-STT-Engine-venv/Py-STT-Engine/audioData/Animals/bird/\"\n",
    "output_path=\"C:/Main/Dev/Github/Py-STT-Engine-venv/Py-STT-Engine/mfccs_all/bird/\"\n",
    "\n",
    "prepare_training_data(input_path,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9998833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d89a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make array of MFCC images of word \"bird\"\n",
    "image_bird_path=\"mfccs_all/bird/\"\n",
    "images_bird=[]\n",
    "\n",
    "image_names=os.listdir(image_bird_path)\n",
    "for im in image_names:\n",
    "    image=cv2.imread(image_bird_path+im)\n",
    "    images_bird.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d8a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make array of MFCC images of word \"cat\"\n",
    "image_cat_path=\"mfccs_all/cat/\"\n",
    "images_cat=[]\n",
    "\n",
    "image_names=os.listdir(image_cat_path)\n",
    "for im in image_names:\n",
    "    image=cv2.imread(image_cat_path+im)\n",
    "    images_cat.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0452778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make array of MFCC images of word \"dog\"\n",
    "image_dog_path=\"mfccs_all/dog/\"\n",
    "images_dog=[]\n",
    "\n",
    "image_names=os.listdir(image_dog_path)\n",
    "for im in image_names:\n",
    "    image=cv2.imread(image_dog_path+im)\n",
    "    images_dog.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523dec6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
