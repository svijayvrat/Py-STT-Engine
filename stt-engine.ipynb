{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11227512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt                                                 \n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.fftpack import dct\n",
    "from silero_vad import load_silero_vad, read_audio, get_speech_timestamps\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPool2D,Flatten,Dense\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40109455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remake_audio(path):\n",
    "    audio,sampleRate = torchaudio.load(path)\n",
    "    audio=audio.numpy()\n",
    "    audio=np.mean(audio,axis=0)\n",
    "    audioSize=audio.size\n",
    "    rem=audioSize%16000\n",
    "    audio=np.append(audio,[np.int16(0)]*(16000-rem))\n",
    "    path+=\"_temp.wav\"\n",
    "    scipy.io.wavfile.write(path,sampleRate,audio)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "id": "9c06b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the audio samples are single-channel(mono)\n",
    "# Loads audio file from path and returns tuple of (sample rate, audio array)\n",
    "def load_audio(path):\n",
    "    path=remake_audio(path)\n",
    "    sampleRate, audio = scipy.io.wavfile.read(path)\n",
    "    return sampleRate,audio,path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "id": "2f8f6d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time vs. Amplitude plot o original audio, loaded initially.\n",
    "def plot_audio_init(audio):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(audio)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(\"Audio Signal\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6656c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voice-Activity Detection using Silero-VAD.\n",
    "# Detects first instance of speech in an audio.\n",
    "# If an error occurs or speech is not detected, function returns tuple of (-1,audio). \n",
    "# A check on returned sampleRate can be applied after function implementation to check if speech is detected without error, or not.\n",
    "\n",
    "def vad(sampleRate,audio,path):\n",
    "    if sampleRate%16000!=0:\n",
    "        sampleRate=16000\n",
    "    wav = read_audio(path,sampling_rate=sampleRate)\n",
    "    os.remove(path)\n",
    "    model = load_silero_vad()\n",
    "    try:\n",
    "        speech_timestamps = get_speech_timestamps(wav,model,sampling_rate=sampleRate)\n",
    "    except:\n",
    "        print(path,sampleRate)\n",
    "    try:\n",
    "        x=speech_timestamps[0]['start']\n",
    "        y=speech_timestamps[0]['end']\n",
    "        wav=wav[x:y]\n",
    "        audio = wav.detach().cpu().numpy()\n",
    "        sampleRate=len(audio)\n",
    "        return sampleRate,audio\n",
    "    except:\n",
    "        print(path)\n",
    "        return -1,audio\n",
    "\n",
    "#Time vs Amplitude plot of audio after voice-activity isolation\n",
    "def plot_audio_vad(audio):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(audio)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(\"Audio Signal after VAD\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "id": "82c6cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate MFCC, we follow the following steps:\n",
    "# audioInput -> pre-emphasis -> framing -> windowing -> fourier transform -> Inverse Mel Scale Filter Bank -> Log() -> DCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "id": "61880082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-emphasis layer\n",
    "# Amplifies higher frequencies in order to balance the spectrum (higher frequencies have lower energies)\n",
    "def pre_emphasize(sampleRate,audio):\n",
    "    pre_emphasis = 0.97\n",
    "    audio_preemphasized=[]\n",
    "    for i in range(1,sampleRate):\n",
    "        audio_preemphasized=np.append(audio_preemphasized,audio[i]-(audio[i-1]*pre_emphasis))\n",
    "\n",
    "    return audio_preemphasized\n",
    "\n",
    "# Time vs. Amplitude plot of audio after pre-emphasis\n",
    "def plot_audio_pre_emphasis(audio_preemphasized):\n",
    "    # Plot the pre-emphasized signal\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(audio_preemphasized)\n",
    "    plt.title('Pre-emphasized Signal')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "id": "6a796376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framing Layer\n",
    "# Since the audio wave is more than a second, windowing is necesarry in order to fully capture the features and allow for correct\n",
    "# calculations to be performed. Thus, for ease of calculations, we slice the wave.\n",
    "# The signal/wave is separated into sections or frames of 25-30 milliseconds.\n",
    "# Since some parts of the signal are always at the ends of the frames, and we have to perform hamming window, this may result in data loss.\n",
    "# To tackle this, we frame-shift with a stride of 15ms. This ensures that parts of signals get to be in the center of the signal.\n",
    "\n",
    "def frame_audio(sampleRate,audio_preemphasized):\n",
    "    shift_stride=220  # ~10 millisecond of stride\n",
    "    frame_size=650 # ~30 millisecond frame\n",
    "    audio_frames=[]\n",
    "\n",
    "    # Produces 65 audio frames\n",
    "    for i in range(0,sampleRate-frame_size,shift_stride):\n",
    "        audio_frames.append(audio_preemphasized[i:i+frame_size])\n",
    "\n",
    "    return frame_size,audio_frames\n",
    "\n",
    "# Time vs. Amplitude plot of audio after framing\n",
    "def plot_audio_frame(audio_frames):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(audio_frames[2])\n",
    "    plt.title('Framed Signal')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "id": "d048ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windowing Layer\n",
    "# Since sudden increase/decrease of amplitude at the edges of the frames create noisy outcomes, we have to smoothen it.\n",
    "# Thus, we apply hamming window\n",
    "\n",
    "def hamm_audio(sampleRate,frame_size,audio_frames):\n",
    "    hammed_audio=[]\n",
    "    for frame in audio_frames:\n",
    "        temp_hammed_audio=[]\n",
    "        for i in range(0,frame_size):\n",
    "            temp_hammed_audio.append(frame[i]*(0.54-0.46*math.cos(2*math.pi*i/(frame_size-1))))\n",
    "        \n",
    "        hammed_audio.append(temp_hammed_audio)\n",
    "\n",
    "    return hammed_audio\n",
    "\n",
    "# Time vs. Amplitude plot of audio after application of hamming window\n",
    "def plot_audio_hammed(hammed_audio):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(hammed_audio[2])\n",
    "    plt.title('Windowed Signal')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "id": "1ec448d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT(Fast Fourier Transform) Layer\n",
    "# Used to convert time-domain signal to frequency-domain to analyze frequency components of speech.\n",
    "# Output of FFT gives complex frequency spectrum (both magnitude and phase)\n",
    "# Since we only need magnitude, we evaluate the power spectrum from the output of FFT\n",
    "# NFFT specifies number of points for the FFT. The output is NFFT/2 points\n",
    "\n",
    "def pow_spec(hammed_audio):\n",
    "    NFFT=2048\n",
    "    complex_power_spectrums=np.fft.rfft(hammed_audio,NFFT)\n",
    "    power_spectrum=(1/NFFT)*pow(np.abs(complex_power_spectrums),2)\n",
    "    return NFFT,power_spectrum\n",
    "\n",
    "# Frequency vs. Power/Frequency plot of audio after FFT\n",
    "def plot_power_spectrum(power_spectrum):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(power_spectrum[2])\n",
    "    plt.title(\"Power Spectral Density\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Power/Frequency (dB/Hz)\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "id": "b2f2eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mel-filter banks\n",
    "# Mel-scale related to human-percieved frequency to its actual frequency. Since humans do not hear sound linearly,\n",
    "# i.e, linear gaps in frequency does not amount to linear change in pitch, we use mel-scale.\n",
    "# Mel-scale is a logarithm scale, which imitates hearing of humans. Thus, it enables us to capture features as if heard by human.\n",
    "\n",
    "# Computing the Mel-Filter bank\n",
    "# 1. Decide upper and lower frequencies in Hertz(SampleRate/2 and 300Hz repectively) \n",
    "# 2. Convert them to mels.\n",
    "# 3. Compute 12 linearly-spaced frequencies inclusive of lower and upper mels.\n",
    "# 4. Convert these points back to Hertz.\n",
    "# 5. Round the frequencies to their nearest FFT Bins.\n",
    "# 6. Create Filterbanks\n",
    "\n",
    "\n",
    "# Creates frequency bins and returns tuple of (number of filters, frequency bins)\n",
    "def mels(sampleRate,NFFT):\n",
    "    freq_to_mel=lambda freq:1125*math.log(1+freq/700)\n",
    "    lower_hz=300\n",
    "    upper_hz=sampleRate/2\n",
    "\n",
    "    lower_mel=freq_to_mel(lower_hz)\n",
    "    upper_mel=freq_to_mel(upper_hz)\n",
    "\n",
    "    n_filters=40\n",
    "    mel_arr=np.linspace(lower_mel,upper_mel,n_filters+2)\n",
    "    hz_arr=[700*(math.exp((i/1125))-1) for i in mel_arr]\n",
    "\n",
    "    freq_bin=[math.floor((NFFT+1)*hz_arr_i/sampleRate) for hz_arr_i in hz_arr]\n",
    "    return n_filters,freq_bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "id": "6f077d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the filterbanks.\n",
    "# Returns filter_banks\n",
    "def mel_filterbanks(NFFT,n_filters,freq_bin,power_spectrum):\n",
    "    temp_filter_bank=np.zeros((n_filters,int((NFFT/2))+1))\n",
    "    for i in range(1,n_filters+1):\n",
    "        for k in range(0,int((NFFT/2))):  #frame length\n",
    "            if k<freq_bin[i]:\n",
    "                temp_filter_bank[i-1][k]=0\n",
    "            elif freq_bin[i-1]<=k and k<=freq_bin[i]:\n",
    "                temp_filter_bank[i-1][k]=(k-freq_bin[i-1])/(freq_bin[i]-freq_bin[i-1])\n",
    "            elif freq_bin[i]<=k and k<=freq_bin[i+1]:\n",
    "                temp_filter_bank[i-1][k]=(freq_bin[i+1]-k)/(freq_bin[i+1]-freq_bin[i])\n",
    "            else:\n",
    "                temp_filter_bank[i-1][k]=0\n",
    "\n",
    "\n",
    "    filter_banks=np.dot(power_spectrum, temp_filter_bank.T)\n",
    "    filter_banks=np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)\n",
    "    filter_banks=np.log(filter_banks+1e-8)\n",
    "\n",
    "    return filter_banks\n",
    "\n",
    "# Mel-Spectogram plot of filter banks\n",
    "def plot_mel_spectogram(sampleRate,filter_banks):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.specshow(filter_banks.T, sr=sampleRate, x_axis='time', y_axis='mel',cmap='turbo')\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Mel-Spectogram\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "id": "d62e2f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate MFCCs\n",
    "# We apply DCT on the filterbanks to obtain a set of 26 Mel-Frequency Cepstral Coefficients.\n",
    "# We only require first 13 coefficients for ASR purposes. Rest are to be discarded.\n",
    "# Returns mfcc\n",
    "def gen_mfcc(filter_banks):\n",
    "    mfcc = dct(filter_banks, type=2, axis=1)[:, 1:13] # Keep 2-13\n",
    "    return mfcc\n",
    "\n",
    "# Plot of first 13 Mel-Frequency Cepstral Coefficients\n",
    "def plot_mfcc(sampleRate,mfcc):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.specshow(mfcc.T, sr=sampleRate, x_axis='time', y_axis='mel',cmap='turbo',vmin=-100,vmax=100)\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Mel-Cepstral Frequency Coefficients\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "e7781f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To simplify the process of getting MFCC:\n",
    "#   get_mfcc(path), returns tuple (flag,mfcc).\n",
    "#       input(String: path)\n",
    "#       gets audio file from the path and applies necessary functions to obtain Mel-Frequency Cepstral Coefficients.\n",
    "#       If, during loading of audio or application of VAD, some error occurs, sampleRate\n",
    "\n",
    "def get_mfcc(path):\n",
    "    flag=0\n",
    "    sampleRate,audio,path=load_audio(path)\n",
    "    sampleRate,audio=vad(sampleRate,audio,path)\n",
    "    if sampleRate==-1:\n",
    "        flag=1\n",
    "        return flag,flag\n",
    "    audio_preemphasized=pre_emphasize(sampleRate,audio)\n",
    "    frame_size,audio_frames=frame_audio(sampleRate,audio_preemphasized)\n",
    "    hammed_audio=hamm_audio(sampleRate,frame_size,audio_frames)\n",
    "    NFFT,power_spectrum=pow_spec(hammed_audio)\n",
    "    n_filters,freq_bin=mels(sampleRate,NFFT)\n",
    "    filter_banks=mel_filterbanks(NFFT,n_filters,freq_bin,power_spectrum)\n",
    "    mfcc=gen_mfcc(filter_banks)\n",
    "    return flag,mfcc\n",
    "\n",
    "def get_mel_filterbanks(path):\n",
    "    sampleRate,audio,path=load_audio(path)\n",
    "    sampleRate,audio=vad(sampleRate,audio,path)\n",
    "    audio_preemphasized=pre_emphasize(sampleRate,audio)\n",
    "    frame_size,audio_frames=frame_audio(sampleRate,audio_preemphasized)\n",
    "    hammed_audio=hamm_audio(sampleRate,frame_size,audio_frames)\n",
    "    NFFT,power_spectrum=pow_spec(hammed_audio)\n",
    "    n_filters,freq_bin=mels(sampleRate,NFFT)\n",
    "    filter_banks=mel_filterbanks(NFFT,n_filters,freq_bin,power_spectrum)\n",
    "    return filter_banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "id": "3150fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts MFCCs from the audio files provided in the input_path and stores the MFCC images at output_path\n",
    "\n",
    "def prepare_data(input_path,output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    file_names = os.listdir(input_path)\n",
    "    for file in file_names:\n",
    "        flag,mfcc=get_mfcc(input_path+file)\n",
    "        if flag==1:\n",
    "            continue\n",
    "        fig=plt.figure(figsize=(1, 1),frameon=False)\n",
    "        librosa.display.specshow(mfcc.T, x_axis='time', y_axis='mel',cmap='turbo',vmin=-100,vmax=100)   \n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path+file+\".png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06380a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audioData/Animals/dog/3f2b358d_nohash_0.wav_temp.wav\n",
      "audioData/Animals/dog/5b09db89_nohash_0.wav_temp.wav\n",
      "audioData/Animals/dog/72198b96_nohash_1.wav_temp.wav\n",
      "audioData/Animals/dog/72d75d96_nohash_1.wav_temp.wav\n",
      "audioData/Animals/dog/84999496_nohash_0.wav_temp.wav\n",
      "audioData/Animals/dog/9ff2d2f4_nohash_0.wav_temp.wav\n",
      "audioData/Animals/dog/acfd3bc3_nohash_0.wav_temp.wav\n",
      "audioData/Animals/dog/d312f481_nohash_1.wav_temp.wav\n",
      "audioData/Animals/dog/e96a5020_nohash_0.wav_temp.wav\n",
      "audioData/Animals/dog/fc28c8d8_nohash_0.wav_temp.wav\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL MUST ONLY BE EXECUTED IF YOU WANT TO EXTRACT MFCCs.\n",
    "# IF THE MFCCs ARE ALREADY STORED, DO NOT RUN THIS CELL. COMMENT THE CODE BELOW\n",
    "\n",
    "#For audio containing word \"dog\"\n",
    "input_path=\"audioData/Animals/dog/\"\n",
    "output_path=\"mfccs_all/dog/\"\n",
    "\n",
    "prepare_data(input_path,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e293b9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audioData/Animals/cat/00f0204f_nohash_2.wav_temp.wav\n",
      "audioData/Animals/cat/095847e4_nohash_0.wav_temp.wav\n",
      "audioData/Animals/cat/0e5193e6_nohash_0.wav_temp.wav\n",
      "audioData/Animals/cat/16db1582_nohash_1.wav_temp.wav\n",
      "audioData/Animals/cat/1bc45db9_nohash_0.wav_temp.wav\n",
      "audioData/Animals/cat/1bc45db9_nohash_1.wav_temp.wav\n",
      "audioData/Animals/cat/1ffd513b_nohash_1.wav_temp.wav\n",
      "audioData/Animals/cat/22aa3665_nohash_0.wav_temp.wav\n",
      "audioData/Animals/cat/32561e9e_nohash_0.wav_temp.wav\n",
      "audioData/Animals/cat/380abbad_nohash_0.wav_temp.wav\n",
      "audioData/Animals/cat/39a6b995_nohash_0.wav_temp.wav\n",
      "audioData/Animals/cat/3ac2e76f_nohash_0.wav_temp.wav\n",
      "audioData/Animals/cat/4ca37738_nohash_0.wav_temp.wav\n",
      "audioData/Animals/cat/6205088b_nohash_0.wav_temp.wav\n",
      "audioData/Animals/cat/62ef962d_nohash_0.wav_temp.wav\n",
      "audioData/Animals/cat/65d14087_nohash_0.wav_temp.wav\n",
      "audioData/Animals/cat/66cff190_nohash_0.wav_temp.wav\n",
      "audioData/Animals/cat/6794a793_nohash_0.wav_temp.wav\n",
      "audioData/Animals/cat/784e281a_nohash_0.wav_temp.wav\n",
      "audioData/Animals/cat/81a345a3_nohash_0.wav_temp.wav\n",
      "audioData/Animals/cat/89865a6f_nohash_1.wav_temp.wav\n",
      "audioData/Animals/cat/8c7c9168_nohash_2.wav_temp.wav\n",
      "audioData/Animals/cat/95ba4996_nohash_1.wav_temp.wav\n",
      "audioData/Animals/cat/a8cf01bc_nohash_1.wav_temp.wav\n",
      "audioData/Animals/cat/bfdb9801_nohash_0.wav_temp.wav\n",
      "audioData/Animals/cat/c53b335a_nohash_0.wav_temp.wav\n",
      "audioData/Animals/cat/d0426d63_nohash_0.wav_temp.wav\n",
      "audioData/Animals/cat/fb24c826_nohash_0.wav_temp.wav\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL MUST ONLY BE EXECUTED IF YOU WANT TO EXTRACT MFCCs.\n",
    "# IF THE MFCCs ARE ALREADY STORED, DO NOT RUN THIS CELL. COMMENT THE CODE BELOW\n",
    "\n",
    "#For audio containing word \"cat\"\n",
    "input_path=\"audioData/Animals/cat/\"\n",
    "output_path=\"mfccs_all/cat/\"\n",
    "\n",
    "prepare_data(input_path,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dcbc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audioData/Animals/bird/1bc45db9_nohash_0.wav_temp.wav\n",
      "audioData/Animals/bird/1bc45db9_nohash_1.wav_temp.wav\n",
      "audioData/Animals/bird/270bfa52_nohash_0.wav_temp.wav\n",
      "audioData/Animals/bird/31270cb2_nohash_0.wav_temp.wav\n",
      "audioData/Animals/bird/43691f67_nohash_4.wav_temp.wav\n",
      "audioData/Animals/bird/44fb4274_nohash_0.wav_temp.wav\n",
      "audioData/Animals/bird/750e3e75_nohash_0.wav_temp.wav\n",
      "audioData/Animals/bird/7e783e3f_nohash_0.wav_temp.wav\n",
      "audioData/Animals/bird/8549f25d_nohash_0.wav_temp.wav\n",
      "audioData/Animals/bird/9f869f70_nohash_1.wav_temp.wav\n",
      "audioData/Animals/bird/a8cf01bc_nohash_0.wav_temp.wav\n",
      "audioData/Animals/bird/b6091c84_nohash_0.wav_temp.wav\n",
      "audioData/Animals/bird/b7a0754f_nohash_0.wav_temp.wav\n",
      "audioData/Animals/bird/cdbd6969_nohash_1.wav_temp.wav\n",
      "audioData/Animals/bird/d9b50b8b_nohash_2.wav_temp.wav\n",
      "audioData/Animals/bird/d9d6559e_nohash_0.wav_temp.wav\n",
      "audioData/Animals/bird/e0322f2c_nohash_0.wav_temp.wav\n",
      "audioData/Animals/bird/e5dadd24_nohash_0.wav_temp.wav\n",
      "audioData/Animals/bird/e652590d_nohash_1.wav_temp.wav\n",
      "audioData/Animals/bird/e71b4ce6_nohash_0.wav_temp.wav\n",
      "audioData/Animals/bird/ffb86d3c_nohash_0.wav_temp.wav\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL MUST ONLY BE EXECUTED IF YOU WANT TO EXTRACT MFCCs.\n",
    "# IF THE MFCCs ARE ALREADY STORED, DO NOT RUN THIS CELL. COMMENT THE CODE BELOW\n",
    "\n",
    "#For audio containing word \"bird\"\n",
    "input_path=\"audioData/Animals/bird/\"\n",
    "output_path=\"mfccs_all/bird/\"\n",
    "\n",
    "prepare_data(input_path,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b809b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the MFCCs are extracted fromt he give audio files, we choose a neural network for classification.\n",
    "# For current usage and for the sake of experiment and learning, we use a Convolutional Neural Network that takes in whole MFCC\n",
    "# plot-image of the image and produces a classification based on them.\n",
    "\n",
    "# However, this method may not be useful when we have an audio file that consists of sentences and not just a single word.\n",
    "# We may need a recurrent neural network for those purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4056e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 551 files belonging to 3 classes.\n",
      "Using 441 files for training.\n",
      "Found 551 files belonging to 3 classes.\n",
      "Using 110 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# # An MFCC plot-image of an audio is of dimensions (100,100,3)\n",
    "# # To pass on the images to a CNN, we create a tensor object from the images and use those tensors to train the CNN model.\n",
    "\n",
    "\n",
    "# # image_dataset_from_directory checks for subdirectories in the path provided, and treats those subdirectories as categories or classes.\n",
    "# # The images inside those subdirectories are used as data of those categories and are turned into tensors.\n",
    "# # validation_split=0.2, which means that 20% of all the data is used as validation data, rest is used as training data.\n",
    "# # subset='training', means that the dataset is to be used for training\n",
    "# # seed=123, seed for shuffling the order\n",
    "# # shuffle=True, to shuffle all the tensors\n",
    "# # labels='inferred', labels are taken from the subdirectories of the path directory.\n",
    "\n",
    "# # Make sure that MFCCs are extracted before running this cell.\n",
    "train_dataset=image_dataset_from_directory('mfccs_all/',validation_split=0.2,subset=\"training\",\n",
    "  seed=123,labels='inferred',image_size=(100,100),batch_size=1,shuffle=True)\n",
    "\n",
    "# subset='validation', means that the dataset is to be used for validation\n",
    "validate_dataset=image_dataset_from_directory('mfccs_all/',validation_split=0.2,subset=\"validation\",\n",
    "  seed=123,labels='inferred',image_size=(100,100),batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac742f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check the first 3 of data.\n",
    "# image tensor(tf.tensor) needs to be converted to numpy array and normalized to range 0-1, for appropriate plot.\n",
    "# label tensor's 0th index contains the label.\n",
    "# 0 = Bird\n",
    "# 1 = Cat\n",
    "# 2 = Dog\n",
    "# num is used to iterate over num images and labels in dataset\n",
    "def check_dataset(dataset,num):\n",
    "    for image_tensor,label_tensor in train_dataset:\n",
    "        if num<=0:\n",
    "            break\n",
    "        image=image_tensor.numpy()\n",
    "        image=image[0]/255\n",
    "        label=label_tensor[0].numpy()\n",
    "        cv2.imshow(f\"{label}\",image)\n",
    "        print(label)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        num-=1\n",
    "\n",
    "    \n",
    "check_dataset(train_dataset,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395fc098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring Convolutional Neural Network to parse and classify MFCC-plots.\n",
    "# Initial Dimensions of an image are (100,100,3)\n",
    "model=Sequential()\n",
    "\n",
    "#Input: (x_dim,y_dim,num_images)\n",
    "#Input: 100, 100, 1\n",
    "model.add(Conv2D(8,(3,3),activation='relu',input_shape=(100,100,3),padding='valid'))\n",
    "model.add(MaxPool2D((2,2),padding='valid'))\n",
    "\n",
    "#Input: 49, 49, 8\n",
    "model.add(Conv2D(16,(3,3),activation='relu',padding='valid'))\n",
    "model.add(MaxPool2D((2,2),padding='valid'))\n",
    "\n",
    "#Input: 23, 23, 16\n",
    "model.add(Conv2D(32,(3,3),activation='relu',padding='valid'))\n",
    "model.add(MaxPool2D((2,2),padding='valid'))\n",
    "\n",
    "#Input: 10, 10, 32\n",
    "model.add(Conv2D(64,(3,3),activation='relu',padding='valid'))\n",
    "model.add(MaxPool2D((2,2),padding='valid'))\n",
    "\n",
    "#Input: 4, 4, 64\n",
    "model.add(Flatten())\n",
    "\n",
    "#Input: 1024 1D-Array\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(3,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "5f84dda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,075</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_36 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │           \u001b[38;5;34m224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_36 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_37 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │         \u001b[38;5;34m1,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_37 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_38 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_38 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_39 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_39 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_9 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m1,049,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │         \u001b[38;5;34m3,075\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,077,203</span> (4.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,077,203\u001b[0m (4.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,077,203</span> (4.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,077,203\u001b[0m (4.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "fcf1ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile multiple models based on prediction values.\n",
    "# # Here, 7 models have been compiled with different configurations of convolutional layers, number of kernels per layer,\n",
    "# # and number of fully connected layers with differing neurons.\n",
    "\n",
    "model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "f4b93ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/441\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 8.9165e-05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m441/441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.6704e-05\n"
     ]
    }
   ],
   "source": [
    "# model was trained for 7-8 epochs\n",
    "history = model.fit(\n",
    "  train_dataset,\n",
    "  epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "7cbf4cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9267 - loss: 0.8156\n"
     ]
    }
   ],
   "source": [
    "loss,accuracy=model.evaluate(validate_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85597ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RUN THIS CELL IF YOU WANT TO SAVE YOUR MODEL\n",
    "# # All the models are saved for later evaluations and comparisons.\n",
    "\n",
    "# model.save('models/model13.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "1c2c451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the saved models and compare the accuracies over 10 iterations of predictions\n",
    "# Results are saved in the results dictionary, with keys as mean accuracy and values as model path.\n",
    "\n",
    "iter=10\n",
    "model_paths=os.listdir('models/')\n",
    "results={}\n",
    "for path in model_paths:\n",
    "    model=tf.keras.models.load_model('models/'+path)\n",
    "    total_acc=0\n",
    "    for i in range(iter):\n",
    "        loss,accuracy=model.evaluate(validate_dataset,verbose=0)\n",
    "        total_acc+=accuracy\n",
    "\n",
    "    results[total_acc/iter]='models/'+path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "id": "cff907b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: models/model12.keras\n",
      "Avg accuracy: 0.9545454382896423\n"
     ]
    }
   ],
   "source": [
    "# Pick the best model\n",
    "best_model=results[max(results.keys())]\n",
    "print(f\"Best model: {best_model}\")\n",
    "print(f\"Avg accuracy: {max(results.keys())}\")\n",
    "model=tf.keras.models.load_model(best_model)\n",
    "model.save('models/best_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "id": "59607896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svija\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:708: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9686 - loss: 0.2503\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test images\n",
    "\n",
    "loss,accuracy=model.evaluate(validate_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745c4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 57344\n",
      "torch.Size([64000])\n",
      "44100 47104\n",
      "torch.Size([17415])\n",
      "48000 48128\n",
      "torch.Size([64000])\n"
     ]
    }
   ],
   "source": [
    "# Now, we predict audio that is not present in the training or validation data.\n",
    "# These audio files were collected from sources on the internet.\n",
    "\n",
    "# Extracting MFCCs from the audio files.\n",
    "prepare_data('test_words/audioData/bird/','test_words/mfccs/bird/')\n",
    "prepare_data('test_words/audioData/cat/','test_words/mfccs/cat/')\n",
    "prepare_data('test_words/audioData/dog/','test_words/mfccs/dog/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c199f78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Make labeled dataset from the MFCCs obtained\n",
    "test_data=image_dataset_from_directory('test_words/mfccs/',\n",
    "  seed=123,labels='inferred',image_size=(100,100),batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5fd32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0124"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0104\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "loss,accuracy=model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad716c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Text from prediction\n",
    "words={\n",
    "    0:\"Bird\",\n",
    "    1:\"Cat\",\n",
    "    2:\"Dog\"\n",
    "}\n",
    "for im,lb in test_data:\n",
    "    pred=model.predict(im,verbose=3)\n",
    "    print(f\"Prediction: {words[np.argmax(pred)]} \\t True Value: {words[lb[0].numpy()]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
