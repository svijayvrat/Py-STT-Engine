{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11227512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.fftpack import dct\n",
    "from silero_vad import load_silero_vad, read_audio, get_speech_timestamps\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPool2D,Flatten,Dense\n",
    "from tensorflow.keras.utils import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c06b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the audio samples are single-channel(mono)\n",
    "# Loads audio file from path and returns tuple of (sample rate, audio array)\n",
    "def load_audio(path):\n",
    "    sampleRate, audio = scipy.io.wavfile.read(path)\n",
    "    audio=audio[0:int(3.5*sampleRate)]\n",
    "    sampleRate=audio.size\n",
    "    return sampleRate,audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f8f6d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time vs. Amplitude plot o original audio, loaded initially.\n",
    "def plot_audio_init(audio):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(audio)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(\"Audio Signal\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea6656c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voice-Activity Detection using Silero-VAD.\n",
    "# Detects first instance of speech in an audio.\n",
    "# If an error occurs or speech is not detected, function returns tuple of (-1,audio). \n",
    "# A check on returned sampleRate can be applied after function implementation to check if speech is detected without error, or not.\n",
    "\n",
    "def vad(sampleRate,audio,path):\n",
    "    model = load_silero_vad()\n",
    "    wav = read_audio(path)\n",
    "    speech_timestamps = get_speech_timestamps(wav,model)\n",
    "    try:\n",
    "        x=speech_timestamps[0]['start']\n",
    "    except:\n",
    "        return -1,audio\n",
    "    if speech_timestamps[0]['start']+6000>sampleRate:\n",
    "        return -1,audio\n",
    "    else:\n",
    "        speech_timestamps[0]['start']+6000\n",
    "    audio=audio[speech_timestamps[0]['start']:speech_timestamps[0]['start']+6000]\n",
    "    sampleRate=audio.size\n",
    "    return sampleRate,audio\n",
    "\n",
    "#Time vs Amplitude plot of audio after voice-activity isolation\n",
    "def plot_audio_vad(audio):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(audio)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(\"Audio Signal after VAD\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82c6cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate MFCC, we follow the following steps:\n",
    "# audioInput -> pre-emphasis -> framing -> windowing -> fourier transform -> Inverse Mel Scale Filter Bank -> Log() -> DCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61880082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-emphasis layer\n",
    "# Amplifies higher frequencies in order to balance the spectrum (higher frequencies have lower energies)\n",
    "def pre_emphasize(sampleRate,audio):\n",
    "    pre_emphasis = 0.97\n",
    "    audio_preemphasized=[]\n",
    "    for i in range(1,sampleRate):\n",
    "        audio_preemphasized=np.append(audio_preemphasized,audio[i]-(audio[i-1]*pre_emphasis))\n",
    "\n",
    "    return audio_preemphasized\n",
    "\n",
    "# Time vs. Amplitude plot of audio after pre-emphasis\n",
    "def plot_audio_pre_emphasis(audio_preemphasized):\n",
    "    # Plot the pre-emphasized signal\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(audio_preemphasized)\n",
    "    plt.title('Pre-emphasized Signal')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a796376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framing Layer\n",
    "# Since the audio wave is more than a second, windowing is necesarry in order to fully capture the features and allow for correct\n",
    "# calculations to be performed. Thus, for ease of calculations, we slice the wave.\n",
    "# The signal/wave is separated into sections or frames of 25-30 milliseconds.\n",
    "# Since some parts of the signal are always at the ends of the frames, and we have to perform hamming window, this may result in data loss.\n",
    "# To tackle this, we frame-shift with a stride of 15ms. This ensures that parts of signals get to be in the center of the signal.\n",
    "\n",
    "def frame_audio(sampleRate,audio_preemphasized):\n",
    "    shift_stride=220  # ~10 millisecond of stride\n",
    "    frame_size=650 # ~30 millisecond frame\n",
    "    audio_frames=[]\n",
    "\n",
    "    # Produces 65 audio frames\n",
    "    for i in range(0,sampleRate-frame_size,shift_stride):\n",
    "        audio_frames.append(audio_preemphasized[i:i+frame_size])\n",
    "\n",
    "    return frame_size,audio_frames\n",
    "\n",
    "# Time vs. Amplitude plot of audio after framing\n",
    "def plot_audio_frame(audio_frames):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(audio_frames[2])\n",
    "    plt.title('Framed Signal')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d048ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windowing Layer\n",
    "# Since sudden increase/decrease of amplitude at the edges of the frames create noisy outcomes, we have to smoothen it.\n",
    "# Thus, we apply hamming window\n",
    "\n",
    "def hamm_audio(sampleRate,frame_size,audio_frames):\n",
    "    hammed_audio=[]\n",
    "    for frame in audio_frames:\n",
    "        temp_hammed_audio=[]\n",
    "        for i in range(0,frame_size):\n",
    "            temp_hammed_audio.append(frame[i]*(0.54-0.46*math.cos(2*math.pi*i/(frame_size-1))))\n",
    "        \n",
    "        hammed_audio.append(temp_hammed_audio)\n",
    "\n",
    "    return hammed_audio\n",
    "\n",
    "# Time vs. Amplitude plot of audio after application of hamming window\n",
    "def plot_audio_hammed(hammed_audio):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(hammed_audio[2])\n",
    "    plt.title('Windowed Signal')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ec448d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT(Fast Fourier Transform) Layer\n",
    "# Used to convert time-domain signal to frequency-domain to analyze frequency components of speech.\n",
    "# Output of FFT gives complex frequency spectrum (both magnitude and phase)\n",
    "# Since we only need magnitude, we evaluate the power spectrum from the output of FFT\n",
    "# NFFT specifies number of points for the FFT. The output is NFFT/2 points\n",
    "\n",
    "def pow_spec(hammed_audio):\n",
    "    NFFT=2048\n",
    "    complex_power_spectrums=np.fft.rfft(hammed_audio,NFFT)\n",
    "    power_spectrum=(1/NFFT)*pow(np.abs(complex_power_spectrums),2)\n",
    "    return NFFT,power_spectrum\n",
    "\n",
    "# Frequency vs. Power/Frequency plot of audio after FFT\n",
    "def plot_power_spectrum(power_spectrum):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(power_spectrum[2])\n",
    "    plt.title(\"Power Spectral Density\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Power/Frequency (dB/Hz)\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2f2eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mel-filter banks\n",
    "# Mel-scale related to human-percieved frequency to its actual frequency. Since humans do not hear sound linearly,\n",
    "# i.e, linear gaps in frequency does not amount to linear change in pitch, we use mel-scale.\n",
    "# Mel-scale is a logarithm scale, which imitates hearing of humans. Thus, it enables us to capture features as if heard by human.\n",
    "\n",
    "# Computing the Mel-Filter bank\n",
    "# 1. Decide upper and lower frequencies in Hertz(SampleRate/2 and 300Hz repectively) \n",
    "# 2. Convert them to mels.\n",
    "# 3. Compute 12 linearly-spaced frequencies inclusive of lower and upper mels.\n",
    "# 4. Convert these points back to Hertz.\n",
    "# 5. Round the frequencies to their nearest FFT Bins.\n",
    "# 6. Create Filterbanks\n",
    "\n",
    "\n",
    "# Creates frequency bins and returns tuple of (number of filters, frequency bins)\n",
    "def mels(sampleRate,NFFT):\n",
    "    freq_to_mel=lambda freq:1125*math.log(1+freq/700)\n",
    "    lower_hz=300\n",
    "    upper_hz=sampleRate/2\n",
    "\n",
    "    lower_mel=freq_to_mel(lower_hz)\n",
    "    upper_mel=freq_to_mel(upper_hz)\n",
    "\n",
    "    n_filters=40\n",
    "    mel_arr=np.linspace(lower_mel,upper_mel,n_filters+2)\n",
    "    hz_arr=[700*(math.exp((i/1125))-1) for i in mel_arr]\n",
    "\n",
    "    freq_bin=[math.floor((NFFT+1)*hz_arr_i/sampleRate) for hz_arr_i in hz_arr]\n",
    "    return n_filters,freq_bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f077d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the filterbanks.\n",
    "# Returns filter_banks\n",
    "def mel_filterbanks(NFFT,n_filters,freq_bin,power_spectrum):\n",
    "    temp_filter_bank=np.zeros((n_filters,int((NFFT/2))+1))\n",
    "    for i in range(1,n_filters+1):\n",
    "        for k in range(0,int((NFFT/2))):  #frame length\n",
    "            if k<freq_bin[i]:\n",
    "                temp_filter_bank[i-1][k]=0\n",
    "            elif freq_bin[i-1]<=k and k<=freq_bin[i]:\n",
    "                temp_filter_bank[i-1][k]=(k-freq_bin[i-1])/(freq_bin[i]-freq_bin[i-1])\n",
    "            elif freq_bin[i]<=k and k<=freq_bin[i+1]:\n",
    "                temp_filter_bank[i-1][k]=(freq_bin[i+1]-k)/(freq_bin[i+1]-freq_bin[i])\n",
    "            else:\n",
    "                temp_filter_bank[i-1][k]=0\n",
    "\n",
    "\n",
    "    filter_banks=np.dot(power_spectrum, temp_filter_bank.T)\n",
    "    filter_banks=np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)\n",
    "    filter_banks=np.log(filter_banks+1e-8)\n",
    "\n",
    "    return filter_banks\n",
    "\n",
    "# Mel-Spectogram plot of filter banks\n",
    "def plot_mel_spectogram(sampleRate,filter_banks):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.specshow(filter_banks.T, sr=sampleRate, x_axis='time', y_axis='mel',cmap='turbo')\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Mel-Spectogram\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d62e2f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate MFCCs\n",
    "# We apply DCT on the filterbanks to obtain a set of 26 Mel-Frequency Cepstral Coefficients.\n",
    "# We only require first 13 coefficients for ASR purposes. Rest are to be discarded.\n",
    "# Returns mfcc\n",
    "def gen_mfcc(filter_banks):\n",
    "    mfcc = dct(filter_banks, type=2, axis=1)[:, 1:13] # Keep 2-13\n",
    "    return mfcc\n",
    "\n",
    "# Plot of first 13 Mel-Frequency Cepstral Coefficients\n",
    "def plot_mfcc(sampleRate,mfcc):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.specshow(mfcc.T, sr=sampleRate, x_axis='time', y_axis='mel',cmap='turbo',vmin=-100,vmax=100)\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Mel-Cepstral Frequency Coefficients\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7781f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To simplify the process of getting MFCC:\n",
    "#   get_mfcc(path), returns tuple (flag,mfcc).\n",
    "#       input(String: path)\n",
    "#       gets audio file from the path and applies necessary functions to obtain Mel-Frequency Cepstral Coefficients.\n",
    "#       If, during loading of audio or application of VAD, some error occurs, sampleRate\n",
    "\n",
    "def get_mfcc(path):\n",
    "    flag=0\n",
    "    sampleRate,audio=load_audio(path)\n",
    "    sampleRate,audio=vad(sampleRate,audio,path)\n",
    "    if sampleRate==-1:\n",
    "        flag=1\n",
    "        return flag,flag\n",
    "    audio_preemphasized=pre_emphasize(sampleRate,audio)\n",
    "    frame_size,audio_frames=frame_audio(sampleRate,audio_preemphasized)\n",
    "    hammed_audio=hamm_audio(sampleRate,frame_size,audio_frames)\n",
    "    NFFT,power_spectrum=pow_spec(hammed_audio)\n",
    "    n_filters,freq_bin=mels(sampleRate,NFFT)\n",
    "    filter_banks=mel_filterbanks(NFFT,n_filters,freq_bin,power_spectrum)\n",
    "    mfcc=gen_mfcc(filter_banks)\n",
    "    return flag,mfcc\n",
    "\n",
    "def get_mel_filterbanks(path):\n",
    "    sampleRate,audio=load_audio(path)\n",
    "    sampleRate,audio=vad(sampleRate,audio,path)\n",
    "    audio_preemphasized=pre_emphasize(sampleRate,audio)\n",
    "    frame_size,audio_frames=frame_audio(sampleRate,audio_preemphasized)\n",
    "    hammed_audio=hamm_audio(sampleRate,frame_size,audio_frames)\n",
    "    NFFT,power_spectrum=pow_spec(hammed_audio)\n",
    "    n_filters,freq_bin=mels(sampleRate,NFFT)\n",
    "    filter_banks=mel_filterbanks(NFFT,n_filters,freq_bin,power_spectrum)\n",
    "    return filter_banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3150fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(input_path,output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    file_names = os.listdir(input_path)\n",
    "    for file in file_names:\n",
    "        flag,mfcc=get_mfcc(input_path+file)\n",
    "        if flag==1:\n",
    "            continue\n",
    "        fig=plt.figure(figsize=(1, 1),frameon=False)\n",
    "        librosa.display.specshow(mfcc.T, x_axis='time', y_axis='mel',cmap='turbo',vmin=-100,vmax=100)   \n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path+file+\".png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06380a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #For audio containing word \"dog\"\n",
    "# input_path=\"C:/Main/Dev/Github/Py-STT-Engine-venv/Py-STT-Engine/audioData/Animals/dog/\"\n",
    "# output_path=\"C:/Main/Dev/Github/Py-STT-Engine-venv/Py-STT-Engine/mfccs_all/dog/\"\n",
    "\n",
    "# prepare_training_data(input_path,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e293b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #For audio containing word \"cat\"\n",
    "# input_path=\"C:/Main/Dev/Github/Py-STT-Engine-venv/Py-STT-Engine/audioData/Animals/cat/\"\n",
    "# output_path=\"C:/Main/Dev/Github/Py-STT-Engine-venv/Py-STT-Engine/mfccs_all/cat/\"\n",
    "\n",
    "# prepare_training_data(input_path,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7dcbc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #For audio containing word \"bird\"\n",
    "# input_path=\"C:/Main/Dev/Github/Py-STT-Engine-venv/Py-STT-Engine/audioData/Animals/bird/\"\n",
    "# output_path=\"C:/Main/Dev/Github/Py-STT-Engine-venv/Py-STT-Engine/mfccs_all/bird/\"\n",
    "\n",
    "# prepare_training_data(input_path,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b809b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the MFCCs are extracted fromt he give audio files, we choose a neural network for classification.\n",
    "# For current usage and for the sake of experiment and learning, we use a Convolutional Neural Network that takes in whole MFCC\n",
    "# plot-image of the image and produces a classification based on them.\n",
    "\n",
    "# However, this method may not be useful when we have an audio file that consists of sentences and not just a single word.\n",
    "# We may need a recurrent neural network for those purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4056e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An MFCC plot-image of an audio is of dimensions (400,1200,3)\n",
    "# To pass on the images to a CNN, we create a tensor object from the images and use those tensors to train the CNN model.\n",
    "\n",
    "\n",
    "# image_dataset_from_directory checks for subdirectories in the path provided, and treats those subdirectories as categories or classes.\n",
    "# The images inside those subdirectories are used as data of those categories and are turned into tensors.\n",
    "# validation_split=0.2, which means that 20% of all the data is used as validation data, rest is used as training data.\n",
    "# subset='training', means that the dataset is to be used for training\n",
    "# seed=123, seed for shuffling the order\n",
    "# shuffle=True, to shuffle all the tensors\n",
    "# labels='inferred', labels are taken from the subdirectories of the path directory.\n",
    "train_dataset=image_dataset_from_directory('mfccs_all/',validation_split=0.2,subset=\"training\",\n",
    "  seed=123,labels='inferred',image_size=(100,100),batch_size=1,shuffle=True)\n",
    "\n",
    "# subset='validation', means that the dataset is to be used for validation\n",
    "validate_dataset=image_dataset_from_directory('mfccs_all/',validation_split=0.2,subset=\"validation\",\n",
    "  seed=123,labels='inferred',image_size=(100,100),batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac742f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data.\n",
    "# image tensor(tf.tensor) needs to be converted to numpy array and normalized to range 0-1, for appropriate plot.\n",
    "# label tensor's 0th index contains the label.\n",
    "# 0 = Bird\n",
    "# 1 = Cat\n",
    "# 2 = Dog\n",
    "# num is used to iterate over num images and labels in dataset\n",
    "def check_dataset(dataset,num):\n",
    "    for image_tensor,label_tensor in train_dataset:\n",
    "        if num<=0:\n",
    "            break\n",
    "        image=image_tensor.numpy()\n",
    "        image=image[0]/255\n",
    "        label=label_tensor[0].numpy()\n",
    "        cv2.imshow(f\"{label}\",image)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        num-=1\n",
    "\n",
    "    \n",
    "# check_dataset(train_dataset,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395fc098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring Convolutional Neural Network to parse and classify MFCC-plots.\n",
    "# Initial Dimensions of an image are (100,100,3)\n",
    "model=Sequential()\n",
    "\n",
    "#Input: (x_dim,y_dim,num_images)\n",
    "#Input: 100, 100, 1\n",
    "model.add(Conv2D(8,(3,3),activation='relu',input_shape=(100,100,3),padding='valid'))\n",
    "model.add(MaxPool2D((2,2),padding='valid'))\n",
    "\n",
    "#Input: 49, 49, 8\n",
    "model.add(Conv2D(16,(3,3),activation='relu',padding='valid'))\n",
    "model.add(MaxPool2D((2,2),padding='valid'))\n",
    "\n",
    "#Input: 23, 23, 16\n",
    "model.add(Conv2D(32,(3,3),activation='relu',padding='valid'))\n",
    "model.add(MaxPool2D((2,2),padding='valid'))\n",
    "\n",
    "#Input: 10, 10, 32\n",
    "model.add(Conv2D(64,(3,3),activation='relu',padding='valid'))\n",
    "model.add(MaxPool2D((2,2),padding='valid'))\n",
    "\n",
    "#Input: 4, 4, 64\n",
    "model.add(Flatten())\n",
    "\n",
    "#Input: 1024 1D-Array\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(200,activation='relu'))\n",
    "model.add(Dense(3,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f84dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf1ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile multiple models based on prediction values.\n",
    "# Here, 7 models have been compiled with different configurations of convolutional layers, number of kernels per layer,\n",
    "# and number of fully connected layers with differing neurons.\n",
    "\n",
    "model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b93ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "  train_dataset,\n",
    "  epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbf4cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy=model.evaluate(validate_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85597ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the models are saved for later evaluations and comparisons.\n",
    "\n",
    "# model.save('models/cnn7.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the saved models and compare the accuracies over 10 iterations of predictions\n",
    "# Results are saved in the results dictionary, with keys as mean accuracy and values as model path.\n",
    "\n",
    "iter=10\n",
    "model_paths=os.listdir('models/')\n",
    "results={}\n",
    "for path in model_paths:\n",
    "    model=tf.keras.models.load_model('models/'+path)\n",
    "    total_acc=0\n",
    "    for i in range(iter):\n",
    "        loss,accuracy=model.evaluate(validate_dataset,verbose=0)\n",
    "        total_acc+=accuracy\n",
    "\n",
    "    results[total_acc/iter]='models/'+path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff907b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the best model\n",
    "best_model=results[max(results.keys())]\n",
    "print(f\"Best model: {best_model}\")\n",
    "print(f\"Avg accuracy: {max(results.keys())}\")\n",
    "model=tf.keras.models.load_model(best_model)\n",
    "model.save('best_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59607896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test images\n",
    "\n",
    "loss,accuracy=model.evaluate(validate_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
